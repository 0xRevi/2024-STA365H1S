{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 7: (a) Posterior Predictive Distributions<br> and (b) Missing Data Imputation\n",
        "\n",
        "Student: Ante Malenica"
      ],
      "metadata": {
        "id": "N54FMFbG3fWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Describe how the posterior predictive distribution is created for mixture models"
      ],
      "metadata": {
        "id": "CkvZUjFi3zS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundamentally, mixture models use the combination of subdistributions to create flexible modelling. This technique is supported by the outcome where mixture distributions':\n",
        "> \"density functions are able to approximate\n",
        "the density function of any unknown distribution to arbitrary degrees of accuracy, provided that the mixing or mixture\n",
        "distribution is sufficiently complex.\" [[Source]](https://arxiv.org/pdf/1611.03974.pdf#:~:text=An%20often%2Dcited%20fact%20regarding,is%20often%20not%20made%20concrete)\n",
        "\n",
        "\n",
        "From this, posterior predictive distributions are created over numerous iterations where first, a random **chain** and **draw** is picked for all relevant parameters (say $\\mu$ and $\\sigma$ for a normal distribution) defining the mixture model. Then, define a mixture distribution (in this context, a normal distribution) dependant on the relevant parameters ($\\mu$ and $\\sigma$). Draw the mixture distribution with respect to the posterior weights, indexed by the chain and draws for this iteration, and compute its sum.\n",
        "\n",
        "This process compliments the one outlined in [\"(MIxture Model) Posterior Predictive Distributions [8 minutes]\"](https://github.com/pointOfive/STA365_W24_Bayes/blob/main/Bayes8_DirichletProcessMixtureModels.ipynb)."
      ],
      "metadata": {
        "id": "Ye_Y2wetx_u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Describe how the posterior predictive distribution is created in general"
      ],
      "metadata": {
        "id": "CJO7d8tN32xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, given some prior beliefs and parameters $\\theta$, one needs to sample $\\theta$ for the model. These sampled parameters can then compute $p(y_{\\text{new}} | \\theta)$, for some new predicted unobserved data, $y_{\\text{new}}$. Over some iterations, numerous computations for the posterior distribution are made, and then the posterior predictive distribution $p(y_{\\text{new}} | \\text{data})$ is computed as follows,\n",
        "$$\n",
        "p(y_{\\text{new}} | \\text{data}) = \\int p(y_{\\text{new}} | \\theta) \\, p(\\theta | \\text{data}) \\, d\\theta\n",
        "$$\n"
      ],
      "metadata": {
        "id": "E5oHBuxGFUpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
        "\n",
        "- **Hint: latent variables $v$ indicating the subpopulation are completely missing values that we simply treat as parameters to be inferred through posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**"
      ],
      "metadata": {
        "id": "SGhVMhOR36hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reference to the following updated (and related) material:\n",
        "- https://www.pymc.io/projects/examples/en/latest/howto/Missing_Data_Imputation.html,\n",
        "\n",
        "bayesian data imputation can be used to modify the missing entries of $X$ to perform a regression $y$. Certain techniques may be better for different missing data specifications (MCAR, MAR, or MNAR), as each classification assumes different reasons for the missingness in $X$.\n",
        "\n",
        "For example, MCAR assumptions on missing values in $X$ may make Bayesian imputation techniques biased as the MCAR assumption assumes missingness is a result of some unobserved data (i.e., not explicitly defined in $X$). Hence, context-driven imputation depending on observed data may not accurately reflect this reason for missingness.\n",
        "\n",
        "One can also use bootstrapping sensitivity analysis for estimating parameters against boostrapped samples under the different missingness assumptions as well as general EDA on model fit to discern the validity of different imputation processes.\n",
        "\n",
        "Under appropriate missingness assumptions, exploratory methods like analyzing the pattern of missingness and testing features against the observed data or replacing missing values with posterior predictive distribution values can be used to justifiably reason replacing these missing entries."
      ],
      "metadata": {
        "id": "QvyNwSu335EC"
      }
    }
  ]
}